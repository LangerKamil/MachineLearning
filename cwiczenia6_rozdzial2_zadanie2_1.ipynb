{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LangerKamil/MachineLearning/blob/main/cwiczenia6_rozdzial2_zadanie2_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TZefME0MTvA"
      },
      "source": [
        "# note this runtime does not require a GPU, so make sure the runtime type is set to CPU to lower your Colab usage\n",
        "#!pip install gym==0.17.3.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scCHk8TKrA-D"
      },
      "source": [
        "# imports\n",
        "import gym\n",
        "import copy\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.font_manager\n",
        "plt.rcParams[\"font.family\"] = \"OpenSans\"\n",
        "from gym.envs.toy_text.frozen_lake import generate_random_map"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tMCdMu-RehW"
      },
      "source": [
        "# import the frozen lake gym environment\n",
        "name = 'FrozenLake-v0'\n",
        " \n",
        "random_map = generate_random_map(size=8, p=0.8)\n",
        "env = gym.make(\"FrozenLake-v0\", desc=random_map, is_slippery=False) # warning: setting slippery=True results in very complex environment dynamics where the optimal solution does not make sense to humans!\n",
        "\n",
        "env.seed(742)\n",
        "env.action_space.seed(742)\n",
        "\n",
        "# lets examine it\n",
        "print('action space: ' + str(env.action_space))\n",
        "print('reward range: ' + str(env.reward_range))\n",
        "print('observation space: ' + str(env.observation_space))\n",
        "env.render()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSJzRcjHdPNg"
      },
      "source": [
        "# actions\n",
        "LEFT, DOWN, RIGHT, UP = 0,1,2,3\n",
        "\n",
        "# lets do an example step for the policy\n",
        "env.reset()\n",
        "next_state, reward, terminated, info = env.step(DOWN)\n",
        "print('=============')\n",
        "print('next state: ' + str(next_state))\n",
        "print('terminated: ' + str(terminated))\n",
        "print('    reward: ' + str(reward))\n",
        "print('      info: ' + str(info))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99LTmNVAtWO-"
      },
      "source": [
        "# helper function to draw the frozen lake\n",
        "def plot(V,policy,col_ramp=1,dpi=175,draw_vals=False):\n",
        "    plt.rcParams['figure.dpi'] = dpi\n",
        "    plt.rcParams.update({'axes.edgecolor': (0.32,0.36,0.38)})\n",
        "    plt.rcParams.update({'font.size': 4 if env.env.nrow >= 6 else 7})\n",
        "    plt.figure(figsize=(3,3))\n",
        "    plt.imshow(1-V.reshape(env.env.nrow,env.env.ncol)**col_ramp, cmap='gray', interpolation='none', clim=(0,1))\n",
        "    ax = plt.gca()\n",
        "    ax.set_xticks(np.arange(env.env.ncol)-.5)\n",
        "    ax.set_yticks(np.arange(env.env.nrow)-.5)\n",
        "    ax.set_xticklabels([])\n",
        "    ax.set_yticklabels([])\n",
        "    for s in range(env.nS):\n",
        "        x = s%env.env.nrow\n",
        "        y = int(s/env.env.ncol)\n",
        "        a = policy[s]\n",
        "        gray = np.array((0.32,0.36,0.38))\n",
        "        if env.desc.tolist()[y][x] == b'G': \n",
        "            plt.text(x-0.45,y-0.3, 'goal', color=(0.75,0.22,0.17), fontname='OpenSans', weight='bold')\n",
        "            continue\n",
        "        if a[0] > 0.0: plt.arrow(x, y, float(a[0])*-.84, 0.0, color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # left\n",
        "        if a[1] > 0.0: plt.arrow(x, y, 0.0, float(a[1])*.84,  color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # down\n",
        "        if a[2] > 0.0: plt.arrow(x, y, float(a[2])*.84, 0.0,  color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # right\n",
        "        if a[3] > 0.0: plt.arrow(x, y, 0.0, float(a[3])*-.84, color=gray+0.2*(1-V[s]), head_width=0.1, head_length=0.1) # up\n",
        "        if env.desc.tolist()[y][x] == b'F': plt.text(x-0.45,y-0.3, 'ice', color=(gray*V[s]), fontname='OpenSans')\n",
        "        if env.desc.tolist()[y][x] == b'S': plt.text(x-0.45,y-0.3, 'start',color=(0.21,0.51,0.48), fontname='OpenSans', weight='bold')\n",
        "        if draw_vals and V[s]>0:\n",
        "            vstr = '{0:.1e}'.format(V[s]) if env.env.nrow == 8 else '{0:.6f}'.format(V[s])\n",
        "            plt.text(x-0.45,y+0.45, vstr, color=(gray*V[s]), fontname='OpenSans')\n",
        "    plt.grid(color=(0.42,0.46,0.48), linestyle=':')\n",
        "    ax.set_axisbelow(True)\n",
        "    ax.tick_params(color=(0.42,0.46,0.48),which='both',top='off',left='off',right='off',bottom='off')\n",
        "    plt.show()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbrlLdqdQuvV"
      },
      "source": [
        "**Policy evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQs223bsQwlZ"
      },
      "source": [
        "def policy_evaluation(env, policy, gamma=1, theta=1e-8, draw=False):\n",
        "    V = np.zeros(env.nS)\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.nS):\n",
        "            Vs = 0\n",
        "            for a, action_prob in enumerate(policy[s]):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
        "            delta = max(delta, np.abs(V[s]-Vs))\n",
        "            V[s] = Vs\n",
        "        if draw: plot(V,policy,draw_vals=True)\n",
        "        if delta < theta:\n",
        "            break\n",
        "    return V"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uisqFVaeOf-Y"
      },
      "source": [
        "# lets start with a random policy, in this case there's a 1/4 probability of taking any action at every 4x4 state\n",
        "policy = np.ones([env.nS, env.nA]) / env.nA"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyiCeOi41eI0"
      },
      "source": [
        "# evaluate this policy (change draw=True to show steps, and ensure environment is 'FrozenLake-v0' for the exact same steps in the lecture)\n",
        "V = policy_evaluation(env,policy,draw=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri8LXMhMZQH1"
      },
      "source": [
        "Get $q_\\pi$ form $v_\\pi$ by a one-step look ahead\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZES6iqTZL0A"
      },
      "source": [
        "def q_from_v(env, V, s, gamma=1):\n",
        "    q = np.zeros(env.nA)\n",
        "    for a in range(env.nA):\n",
        "        for prob, next_state, reward, done in env.P[s][a]:\n",
        "            q[a] += prob * (reward + gamma * V[next_state])\n",
        "    return q"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x11slZ9LaR5K"
      },
      "source": [
        "def policy_improvement(env, V, gamma=1):\n",
        "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
        "    for s in range(env.nS):\n",
        "        q = q_from_v(env, V, s, gamma)\n",
        "        \n",
        "        # deterministic policy, will always choose an action (not capture the distribution)\n",
        "        # policy[s][np.argmax(q)] = 1\n",
        "        \n",
        "        # stochastic policy that puts equal probability on maximizing actions\n",
        "        best_a = np.argwhere(q==np.max(q)).flatten()\n",
        "        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n",
        "        \n",
        "    return policy"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mla0JhPK3v70"
      },
      "source": [
        "# plot the policy from a single greedy improvement step after policy evaluation\n",
        "policy = policy_improvement(env, V, gamma = 0.1)\n",
        "plot(V,policy,1.0,draw_vals=True)\n",
        "plot(V,policy,1.0,draw_vals=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDtOnnMfwZ9S"
      },
      "source": [
        "# now lets do value iteration, which is the k=1 case but simplifies saving computation\n",
        "# note how there are no intermediate policies until the end \n",
        "def value_iteration(env, gamma=1, theta=1e-8):\n",
        "    V = np.zeros(env.nS) # initial state value function\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.nS):\n",
        "            v_s = V[s] # store old value\n",
        "            q_s = q_from_v(env, V, s, gamma) # the action value function is calculated for all actions\n",
        "            V[s] = max(q_s) # the next value of the state function is the maximum of all action values\n",
        "            delta = max(delta, abs(V[s] - v_s))\n",
        "        if delta < theta: break\n",
        "    # lastly, at convergence, we can get a (optimal) policy from the optimal state value function\n",
        "    policy = policy_improvement(env, V, gamma)    \n",
        "    return policy, V"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pz8iNp84ah4s"
      },
      "source": [
        "# now solve the MDP by policy iteration\n",
        "def policy_iteration(env, gamma=1, theta=1e-8):\n",
        "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
        "    while True:\n",
        "\n",
        "        # evaluate the policy (get the value function)\n",
        "        V = policy_evaluation(env, policy, gamma, theta)\n",
        "\n",
        "        # greedily choose the best action\n",
        "        new_policy = policy_improvement(env, V)\n",
        "\n",
        "        if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta*1e2:\n",
        "           break;\n",
        "        \n",
        "        policy = copy.copy(new_policy)\n",
        "    return policy, V"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRQUre8JapKJ"
      },
      "source": [
        "# do policy iteration\n",
        "policy_pi, V_pi = policy_iteration(env, gamma=0.25)\n",
        "plot(V_pi,policy_pi,1.0,draw_vals=True)\n",
        "plot(V_pi,policy_pi,1.0,draw_vals=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-fup7K2yCFB"
      },
      "source": [
        "policy_pi, V_pi = value_iteration(env, gamma=1)\n",
        "plot(V_pi,policy_pi,1.0,draw_vals=True)\n",
        "plot(V_pi,policy_pi,1.0,draw_vals=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOpdvpzXX-rE"
      },
      "source": [
        "# here's an expanded version of the above that's more similar to lecture slides\n",
        "def value_iteration_2(env, gamma=1, theta=1e-8):\n",
        "    V = np.zeros(env.nS) \n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(env.nS):\n",
        "            v_s = V[s] \n",
        "\n",
        "            # one step look ahead to get q from v\n",
        "            q_s = np.zeros(env.nA)\n",
        "            for a in range(env.nA):\n",
        "                for prob, next_state, reward, done in env.P[s][a]:\n",
        "                    q_s[a] += prob * (reward + gamma * V[next_state])\n",
        "\n",
        "            V[s] = max(q_s) \n",
        "            delta = max(delta, abs(V[s] - v_s))\n",
        "        if delta < theta: break\n",
        "        \n",
        "    policy = policy_improvement(env, V, gamma)\n",
        "    return policy, V"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XJuEzHXZezS"
      },
      "source": [
        "policy_pi, V_pi = value_iteration_2(env, gamma=0.7)\n",
        "plot(V_pi,policy_pi,1.0,draw_vals=True)\n",
        "plot(V_pi,policy_pi,1.0,draw_vals=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}